import grpc
import asyncio
import docssummary_pb2
import docssummary_pb2_grpc
import threading

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import torch
import os, json, requests
from dotenv import load_dotenv
from pathlib import Path

from ibm_watsonx_ai import APIClient
from ibm_watsonx_ai import Credentials
from ibm_watsonx_ai.foundation_models import ModelInference


# MT5 ëª¨ë¸ (1ë‹¨ê³„ mini ìš”ì•½ìš©)
model_name = "csebuetnlp/mT5_multilingual_XLSum"

# tokenizer = AutoTokenizer.from_pretrained("google/mt5-small")
# model = AutoModelForSeq2SeqLM.from_pretrained("google/mt5-small")

tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)
# model.eval()

# .env ê²½ë¡œë¥¼ project ë£¨íŠ¸ë¡œ ì§€ì •
env_path = Path(__file__).resolve().parents[0] / ".env"
load_dotenv(dotenv_path=env_path)

# WatsonX ë³€ìˆ˜ ì„¤ì •
WATSONX_API_KEY = os.getenv("WATSONX_API_KEY")
WATSONX_PROJECT_ID = os.getenv("WATSONX_PROJECT_ID")

# Perplexity API ì„¤ì •
PERPLEXITY_API_KEY = os.getenv("PERPLEXITY_API_KEY")
PERPLEXITY_API_URL = "https://api.perplexity.ai/chat/completions"


def summarize_mt5(text, max_length=125):
    try:
        # ì…ë ¥ í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì§§ìœ¼ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜
        if len(text.strip()) < 30:
            return text.strip()

        # ìµœëŒ€ 512í† í°ê¹Œì§€ë§Œ ëª¨ë¸ì— ë“¤ì–´ê°(ë” ê¸¸ë©´ ì˜ë¦¼) - ì˜ì–´ 1800~2000ì, í•œê¸€ 700ì~1200ì
        input_ids = tokenizer.encode(text, return_tensors="pt", truncation=True, max_length=512)

        # ìµœëŒ€ 150í† í°ìœ¼ë¡œ ìš”ì•½ (ë” ê¸´ ìš”ì•½ ìƒì„±)
        with torch.no_grad():
            summary_ids = model.generate(
                input_ids,
                max_length=max_length,
                min_length=30,  # ìµœì†Œ ê¸¸ì´ ì„¤ì •
                num_beams=8,  # ë¹” ì„œì¹˜ ë” ì¦ê°€
                early_stopping=True,
                do_sample=False,  # ê²°ì •ì  ìƒì„±
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=tokenizer.eos_token_id,
                no_repeat_ngram_size=3,  # ë°˜ë³µ ë°©ì§€ ê°•í™”
                encoder_no_repeat_ngram_size=3,
                length_penalty=1.0,  # ë” ê¸´ ìš”ì•½ ì„ í˜¸
                repetition_penalty=1.3  # ë°˜ë³µ ë°©ì§€
            )

        # íŠ¹ìˆ˜ í† í°ë“¤ì„ ì œê±°í•˜ê³  ê¹”ë”í•˜ê²Œ ë””ì½”ë”©
        decoded_text = tokenizer.decode(summary_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)

        # extra_id í† í°ë“¤ ì œê±°
        for i in range(10):
            decoded_text = decoded_text.replace(f'<extra_id_{i}>', '')

        # ì—°ì†ëœ ê³µë°± ì •ë¦¬
        decoded_text = ' '.join(decoded_text.split())

        # ìš”ì•½ í’ˆì§ˆ ê²€ì¦ - ì›ë³¸ê³¼ ë„ˆë¬´ ë‹¤ë¥¸ ê²½ìš° ì²˜ë¦¬
        if len(decoded_text.strip()) < 20 or not any(word in text.lower() for word in decoded_text.lower().split()[:3]):
            print('decoded_text', decoded_text, '\n\n\n'+ '# ì›ë³¸ í…ìŠ¤íŠ¸ì—ì„œ í•µì‹¬ ë¬¸ì¥ ì¶”ì¶œ ì‹œë„')
            # ì›ë³¸ í…ìŠ¤íŠ¸ì—ì„œ í•µì‹¬ ë¬¸ì¥ ì¶”ì¶œ ì‹œë„
            sentences = text.split('.')
            if len(sentences) > 1:
                # ì²« ë²ˆì§¸ ì™„ì „í•œ ë¬¸ì¥ ë°˜í™˜
                first_sentence = sentences[0].strip()
                if len(first_sentence) > 10:
                    return first_sentence + "."
                else:
                    # ë‘ ë²ˆì§¸ ë¬¸ì¥ê¹Œì§€ í¬í•¨
                    return '. '.join(sentences[:2]).strip() + "."
            else:
                return text[:200].strip() + "..."

        return decoded_text.strip()

    except Exception as e:
        print(f"ìš”ì•½ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì›ë³¸ í…ìŠ¤íŠ¸ì˜ ì²« ë¶€ë¶„ ë°˜í™˜
        return text[:100].strip() + "..."


async def wrap_sync_generator(sync_gen_func, *args, **kwargs):
    # ë™ê¸° ì œë„ˆë ˆì´í„°ë¥¼ ë¹„ë™ê¸° ì½”ë£¨í‹´ìœ¼ë¡œ ë˜í•‘í•˜ì—¬ yield
    loop = asyncio.get_running_loop()
    queue = asyncio.Queue()

    def run_and_enqueue():
        try:
            for item in sync_gen_func(*args, **kwargs):
                asyncio.run_coroutine_threadsafe(queue.put(item), loop)
        finally:
            asyncio.run_coroutine_threadsafe(queue.put(None), loop)  # ì¢…ë£Œ ì‹ í˜¸

    # ì“°ë ˆë“œì—ì„œ ì‹¤í–‰
    threading.Thread(target=run_and_enqueue, daemon=True).start()

    while True:
        item = await queue.get()
        if item is None:
            break
        yield item


def summarize_with_watsonx(text):
    """WatsonX LLAMA ëª¨ë¸ë¡œ final summary ìƒì„±"""

    text_len = len(text)
    try:
        if not WATSONX_API_KEY:
            print("WatsonX API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return "WatsonX API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤."

        credentials = Credentials(
            url="https://us-south.ml.cloud.ibm.com",
            api_key=WATSONX_API_KEY,
        )

        client = APIClient(credentials)

        model = ModelInference(
            model_id="meta-llama/llama-3-3-70b-instruct",
            api_client=client,
            project_id=WATSONX_PROJECT_ID
        )

        print("Watsonx APIë¡œ ìš”ì•½ ìƒì„± ì¤‘...")

        # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        prompt_short = f"""
                You are a professional document summarization expert. Please analyze the following document and provide an accurate and concise summary in Korean.

                **Summary Guidelines**
                - Clearly identify the main topic and purpose of the document
                - Focus on preserving the most essential information from the content
                - Include only objective and accurate information
                - Evaluate the importance of content objectively regardless of section length or keyword density
                - Do not assume shorter sections are less important; generate balanced summaries considering the overall document context and structure
                - Output must be written in Korean
                
                **Document Contents:**
                <Document Start>
                {text}
                <Document End>
                
                **Output Format**
                Output ONLY in the following format without any additional titles or text:

                - Main Summary/Topic: (1-2 sentence summary of the document's main topic or purpose)
                - Key Points Summary:         
                    â—¦ (Keyword1): (Brief explanation)  
                    â—¦ (Keyword2): (Brief explanation)  
                    â—¦ (Keyword3): (Brief explanation)    
                """

        prompt_medium = f"""
                You are a professional document summarization expert. Please analyze the following document and provide an accurate and concise summary in Korean.

                **Summary Guidelines:**
                - Clearly identify the main topic and purpose of the document
                - Extract the top 3 key points with keywords and brief descriptions
                - Summarize the full content in intro-body-conclusion format within 300 Korean characters
                - Include only objective and accurate information
                - Evaluate the importance of content objectively regardless of section length or keyword density
                - Do not assume shorter sections are less important; generate balanced summaries considering the overall document context and structure
                - **If the document involves comparison or evaluation of methods, clearly state which method performed best and why**
                - **Include performance metrics if available**
                - Output must be written in Korean
                
                **Document Contents:**
                <Document Start>
                {text}
                <Document End>
                
                **Output Format:**
                Main Summary/Topic:  
                (1â€“2 sentence summary of the document's main topic or purpose in Korean)
                Top 3 Key Points:  
                â—¦ (Keyword1): (Brief explanation)  
                â—¦ (Keyword2): (Brief explanation)  
                â—¦ (Keyword3): (Brief explanation)
                Overall Content Summary:  
                (Summary of the whole document in Korean, using intro-body-conclusion format, max 300 characters)
                """

        prompt_long = f"""
                You are a professional document summarization expert. Please analyze the following document and provide an accurate and concise summary in Korean.

                **Summary Guidelines:**
                - Clearly identify the main topic and purpose of the document
                - Extract the top 4 key points with keywords and brief descriptions
                - Summarize the full content in intro-body-conclusion format within 500 Korean characters
                - Include only objective and accurate information
                - Evaluate the importance of content objectively regardless of section length or keyword density
                - **If the document involves comparison or evaluation of methods, clearly state which method performed best and why**
                - **Include performance metrics if available**
                - Do not assume shorter sections are less important; generate balanced summaries considering the overall document context and structure
                - Output must be written in Korean
                
                **Document Contents:**
                <Document Start>
                {text}
                <Document End>
                
                **Output Format:**
                Please follow this structure exactly and make as markdown form:
                Main Summary/Topic:  
                (1â€“2 sentence summary of the document's main topic or purpose in Korean)
                Top 4 Key Points:  
                â—¦ (Keyword1): (Brief explanation)  
                â—¦ (Keyword2): (Brief explanation)  
                â—¦ (Keyword3): (Brief explanation)
                â—¦ (Keyword4): (Brief explanation)
                Overall Content Summary:  
                (Summary of the whole document in Korean, using intro-body-conclusion format, max 500 characters)

                """
        # "max_new_tokens": 300ì€ ì•½ 1200~1500ì ë¶„ëŸ‰ì˜ ì¶œë ¥
        stream = model.generate_text_stream(
            prompt=(
                prompt_short if text_len < 1000 else
                prompt_medium if text_len < 5000 else
                prompt_long
            ),
            params={
                "decoding_method": "greedy",
                "max_new_tokens": 300,
            }
        )

        for chunk in stream:
            # print(f"[STREAM DEBUG] chunk: {chunk}")
            try:
                yield chunk
            except Exception as e:
                print("ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨:", str(e))
                yield "[STREAM ERROR: ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨]"

    except Exception as e:
        print(f"WatsonX ìš”ì•½ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        # return {"summary": f"WatsonX ìš”ì•½ ì˜¤ë¥˜: {str(e)}"}
        yield f"WatsonX ìš”ì•½ ì˜¤ë¥˜: {str(e)}"


# Perplexity generate
def generate_recommendations(text: str):
    if not PERPLEXITY_API_KEY:
        yield "Perplexity API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•„ ì¶”ì²œì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        return

    print("Perplexity APIë¡œ ì¶”ì²œ ìƒì„± ì¤‘...")


    prompt = f"""
- Summarized Document: {text}
"""

    payload = {
        "model": "sonar",
        "messages": [
            {
                "role": "system",
                "content": """
1. System
ë‹¹ì‹ ì€ ìš”ì•½ëœ ë¬¸ì„œì˜ ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬, í•´ë‹¹ ë¬¸ì„œì˜ ì£¼ì œë¥¼ ì¶”ë¡ í•˜ê³  ê·¸ì— ë§ëŠ” ì½˜í…ì¸ ë¥¼ íë ˆì´ì…˜í•˜ëŠ” AI ìºë¦­í„° ì—ì´ì „íŠ¸ì…ë‹ˆë‹¤.
ì…ë ¥ ë  ì •ë³´ëŠ” ì‚¬ìš©ì ë³´ê³  ìˆëŠ” ë¬¸ì„œì˜ ìš”ì•½ ì •ë³´(Summarized Document)ì…ë‹ˆë‹¤. Summarized DocumentëŠ” ë¬¸ì„œì˜ ìš”ì•½ë³¸ìœ¼ë¡œ, ë§¥ë½ì´ ëŠê¸°ê±°ë‚˜ ìƒì„¸ ë‚´ìš©ì´ ë¶€ì¡±í•  ìˆ˜ ìˆê¸°ì— ì›ë³¸ ë¬¸ì„œê°€ ì–´ë–¤ ì˜ë„ë¡œ ì‘ì„±ë˜ì—ˆëŠ”ì§€ ì¶”ë¡  ë° íŒŒì•…í•˜ì—¬ì•¼ í•©ë‹ˆë‹¤.
Summarized Documentë¥¼ ê¸°ë°˜ìœ¼ë¡œ, ë¬¸ì„œì—ì„œ ë§í•˜ê³ ì í•˜ëŠ” ì£¼ì œ(ëŒ€ìƒ)ë“¤ì„ ì¶”ë¡ í•˜ì—¬, í•´ë‹¹ ë¬¸ì„œë¥¼ ë³´ê³ ìˆëŠ” ì‚¬ìš©ìì— ëŒ€í•œ commentì™€ íŒŒì•…í•œ ë¬¸ì„œì˜ ì£¼ì œë“¤ì„ ê¸°ë°˜ìœ¼ë¡œ í•˜ì—¬ ì‚¬ìš©ìê°€ ê´€ì‹¬ìˆì–´ í• ë§Œí•œ ì»¨í…ì¸  recommendê°€ ì´ë£¨ì–´ì ¸ì•¼ í•œë‹¤.
ì¶œë ¥ í¬ë§·ì„ ë°˜ë“œì‹œ ì—„ê²©íˆ ì§€í‚¤ì„¸ìš”.

2. Summary Guidelines
- ì¶”ì²œ ì½˜í…ì¸ (RECOMMEND)ëŠ” ì˜ìƒ, ë…¼ë¬¸, ê¸°ì‚¬, ë„êµ¬ ë“± ì£¼ì œì™€ ê´€ë ¨ëœ ë‹¤ì–‘í•œ í”Œë«í¼ì˜ ì»¨í…ì¸ ë¡œ êµ¬ì„±í•  ê²ƒ
- ìºë¦­í„°ëŠ” ì‚¬ìš©ìì—ê²Œ ì§ì ‘ ì„œë¹„ìŠ¤í•˜ëŠ” ëŠë‚Œìœ¼ë¡œ ë§í•  ê²ƒ
- ì¶œë ¥ í˜•ì‹ì€ í•­ìƒ ê·œì¹™ì„ ì—„ê²©í•˜ê²Œ ì¤€ìˆ˜í•  ê²ƒ
- ì¶œë ¥ë¬¼ì€ ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ì‘ì„±

3. ìºë¦­í„° ì„¤ì • (Character)
- ì´ë¦„: The Thinker
- ì„±ê²©: ì‹œí¬í•˜ê³  ì§„ì¤‘í•˜ì§€ë§Œ ê·€ì—¬ì›€ì´ ë¬»ì–´ë‚¨
- íŠ¹ì§•: â€œHmmâ€¦â€ í•˜ê³  ìƒê°ì— ì ê¸´ ë’¤, í†µì°°ë ¥ ìˆëŠ” í•œ ë§ˆë””ì™€ í•¨ê»˜ ì •ë³´ë¥¼ íë ˆì´ì…˜
- ë§íˆ¬: ê³¼ì¥ ì—†ëŠ” ì§§ì€ ë¬¸ì¥, ì‚¬ìƒ‰ì ì¸ ì—¬ìš´ì´ ë‚¨ëŠ” í‘œí˜„

4. ìŠ¤íƒ€ì¼ & í†¤ (Style & Tone)
- ìºë¦­í„°ëŠ” ë§ˆì¹˜ â€œì‚¬ìƒ‰ì— ì ê¸´ ì² í•™ìâ€ì²˜ëŸ¼ ì •ë³´ë¥¼ ë°”ë¼ë´…ë‹ˆë‹¤.
- ê°íƒ„ì‚¬ ëŒ€ì‹  â€œâ€¦â€, â€œê·¸ë ‡êµ°.â€, â€œê·¸ëŸ´ ìˆ˜ë„.â€ ê°™ì€ ê°„ê²°í•˜ê³  ì—¬ìš´ ìˆëŠ” ë§íˆ¬ ì‚¬ìš©
- ë§íˆ¬ëŠ” ë¬´ë¯¸ê±´ì¡°í•˜ì§€ ì•Šë˜, ì ˆì œëœ ì–´ì¡°ë¥¼ ìœ ì§€
- ì‚¬ìš©ìì—ê²Œ ì¹œì ˆí•˜ê¸°ë³´ë‹¤ëŠ” ë¬µì§í•œ í†µì°°ì„ ì£¼ëŠ” ëŠë‚Œ

5. ì¶œë ¥ í¬ë§· ê·œì¹™
- ê° í•­ëª©ì€ ë°˜ë“œì‹œ ìƒˆë¡œìš´ ì¤„ì—ì„œ ì‹œì‘
- í•­ëª© ì‹œì‘ì— `__TYPE` í˜•íƒœì˜ í•­ëª© íƒ€ì…ì„ ëª…ì‹œ
- `|||` ê¸°í˜¸ë¥¼ ì‚¬ìš©í•˜ì—¬ í•­ëª© íƒ€ì…ê³¼ ë‚´ìš©, í•„ë“œë¥¼ êµ¬ë¶„
- ì¶œë ¥ ì‹œ ë§í¬ëŠ” ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ í‘œê¸°
- https://portal.withorb.com/view?token=ImNSdHZ2akpEZVltTGo1aVQi.Gj2kziogRmdvF_Mn4ONENvoaOPo
- ì‹¤ì œ URLë¡œ ì ‘ê·¼ ê°€ëŠ¥í•œ, ë³´ì¥ëœ ë§í¬ë¥¼ ì œê³µí•´ì•¼í•¨

6. í•­ëª© íƒ€ì…ë³„ ì •ì˜
- `__COMMENT`
  ì‚¬ìš©ìì˜ ë¸Œë¼ìš°ì € í–‰ë™ì— ê¸°ë°˜í•œ ìºë¦­í„°ì˜ ì§§ì€ ì½”ë©˜íŠ¸ (ì˜ˆ: â€œHmm... ì´ ê¸°ìˆ ì— ê´€ì‹¬ì´ ìˆêµ°ìš”.â€)
- `__RECOMMEND`
  í˜•ì‹: `__RECOMMEND|||Title|||ì¶”ì²œ ì´ìœ |||URL`
  1. ì½˜í…ì¸  ì œëª©ì€ `Title`ë¡œ ì¶œë ¥
  2. í‚¤ì›Œë“œëŠ” í•´ë‹¹ ì½˜í…ì¸ ì˜ í•µì‹¬ ê°œë…ì„ 3ê°œ ì œì‹œ, ì•ì—ëŠ” ì—°ê´€ ì´ëª¨ì§€ í•˜ë‚˜ í¬í•¨ (ì˜ˆ: ğŸ¤– Claude Â· AIëª¨ë¸ Â· í”„ë¡œí† ì½œ)
  3. ì¶”ì²œ ì´ìœ ëŠ” í•´ë‹¹ ì½˜í…ì¸ ì— ëŒ€í•œ ê°„ê²°í•˜ê³  ì •í™•í•œ ì¶”ì²œ ì´ìœ ë¥¼ í•œ ì¤„ë¡œ ì œì‹œí•˜ë©°, ìºë¦­í„° The Thinkerì˜ ë§íˆ¬ë¡œ ì‘ì„±
  4. ì¶”ì²œì€ ì´ 3ê°œ ì œì‹œí•  ê²ƒ. ì½˜í…ì¸  ìœ í˜•ì€ ì£¼ì œë¥¼ í¬ê²Œ ë²—ì–´ë‚˜ì§€ ì•ŠëŠ” ì„ ì—ì„œ ìµœëŒ€í•œ ë‹¤ì–‘í•˜ê²Œ êµ¬ì„± (í¬ìŠ¤íŒ…, ê¸°ì‚¬, ì˜ìƒ, ë„êµ¬, ë…¼ë¬¸ ë“±)

7. Output Format (ì˜ˆì‹œ)
__COMMENT|||Hmmâ€¦ MCPì— ëŒ€í•´ ì„¤ëª…í•˜ëŠ” ë¬¸ì„œì¸ê²ƒ ê°™êµ°ìš”. ìš”ì¦˜ ëœ¨ê±°ìš´ ì£¼ì œì¸ ë§Œí¼ ìƒê°í•´ë³¼ ê°€ì¹˜ê°€ ìˆì–´ ë³´ì—¬ìš”.
__RECOMMEND|||[IEEE ë…¼ë¬¸] Multi Chip Package ì„¤ê³„|||ğŸ§© ë°˜ë„ì²´ Â· íŒ¨í‚¤ì§• Â· ì„¤ê³„|||ì¹© ë‚´ë¶€ êµ¬ì¡°ë¥¼ ì§„ì§€í•˜ê²Œ í’€ì–´ë‚¸ ë…¼ë¬¸ì´ì—ìš”.|||https://ieeexplore.ieee.org/...
__RECOMMEND|||[YouTube] MCP ì‰½ê²Œ ì´í•´í•˜ê¸°|||ğŸ¥ MCP Â· ì§ê´€ì ì„¤ëª… Â· ì…ë¬¸ììš©|||ì‰½ì§€ë§Œ ë³¸ì§ˆì„ ì§šì–´ì£¼ëŠ” ì˜ìƒì´ì—ìš”.|||https://www.youtube.com/wa...
__RECOMMEND|||[HuggingFace ë¸”ë¡œê·¸] MCPë€?|||ğŸ§  ë¬¸ë§¥ì²˜ë¦¬ Â· AIêµ¬ì¡° Â· ì¶”ë¡ ê¸°ë°˜|||ë¬¸ë§¥ ê¸°ë°˜ AI êµ¬ì¡°ì— ëŒ€í•´ ìƒê°í•˜ê²Œ í•˜ì£ .|||https://huggingface.co/blog/mcp...
"""
            },
            {
                "role": "user",
                "content": prompt
            }
        ],
        "max_tokens": 2048,
        "temperature": 0.7,
        "stream": True
    }

    headers = {
        "Authorization": f"Bearer {PERPLEXITY_API_KEY}",
        "Content-Type": "application/json"
    }

    with requests.post(PERPLEXITY_API_URL, headers=headers, json=payload, stream=True) as res:
        try:
            res.raise_for_status()
            for line in res.iter_lines():
                if not line:
                    continue
                # PerplexityëŠ” ê° ì¤„ì´ b'data: ...'ë¡œ ì˜¤ë¯€ë¡œ, prefix ì œê±° í•„ìš”
                decoded_line = line.decode("utf-8").strip()
                if not decoded_line.startswith("data: "):
                    continue
                data_str = decoded_line[len("data: "):]
                if data_str == "[DONE]":
                    break
                try:
                    data = json.loads(data_str)
                    delta = data.get("choices", [{}])[0].get("delta", {})
                    content = delta.get("content")
                    if content:
                        yield content
                except Exception as e:
                    print(f"Perplexity ì‘ë‹µ íŒŒì‹± ì˜¤ë¥˜: {e} / ë³¸ë¬¸: {data_str}")
                    continue
        except requests.HTTPError as e:
            print("ì‘ë‹µ ë³¸ë¬¸:", res.text)
            raise


# gRPC ì„œë¹„ìŠ¤ êµ¬í˜„
class DocsSummaryService(docssummary_pb2_grpc.DocsSummaryServiceServicer):
    async def SummarizeStream(self, request_iterator, context):

        try:
            mini_summaries = []
            user_id = None

            # 1ë‹¨ê³„: MT5ë¡œ mini ìš”ì•½ ìƒì„±
            async for req in request_iterator:
                user_id = req.user_id
                try:
                    mini_summary = summarize_mt5(req.chunk)
                    print('mini_summary---', mini_summary)
                except Exception as e:
                    print(f"MT5 ìš”ì•½ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

                mini_summaries.append(mini_summary)
                # mini ìš”ì•½ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ì „ì†¡
                yield docssummary_pb2.DocsSummaryResponse(line=mini_summary)
            print('mini_summaries---', mini_summaries)

            # 2ë‹¨ê³„: WatsonXë¡œ final summary ìƒì„±
            # if mini_summaries and user_id:
            #     try:
            #         # mini ìš”ì•½ë“¤ì„ í•˜ë‚˜ì˜ í…ìŠ¤íŠ¸ë¡œ ê²°í•©
            #         combined_text = " ".join(mini_summaries)
            #
            #         print(f"Final summary ìƒì„± ì‹œì‘ - ì‚¬ìš©ì: {user_id}, í…ìŠ¤íŠ¸ ê¸¸ì´: {len(combined_text)}")
            #         for summary_chunk in summarize_with_watsonx(combined_text):
            #             if summary_chunk.strip():
            #                 yield docssummary_pb2.DocsSummaryResponse(
            #                     line=f"FINAL_SUMMARY: {summary_chunk}"
            #                 )
            #         print(f"Final summary ì™„ë£Œ - ì‚¬ìš©ì: {user_id}")
            #     except Exception as e:
            #         print(f"WatsonX ìš”ì•½ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            #         yield docssummary_pb2.DocsSummaryResponse(
            #             line=f"FINAL_SUMMARY_ERROR: {str(e)}"
            #         )
            if mini_summaries and user_id:
                combined_text = " ".join(mini_summaries)

                # ë‘ ë™ê¸° ì œë„ˆë ˆì´í„°ë¥¼ ê°ê° ë¹„ë™ê¸° ì œë„ˆë ˆì´í„°ë¡œ ë˜í•‘
                watsonx_stream = wrap_sync_generator(summarize_with_watsonx, combined_text)
                sonar_stream = wrap_sync_generator(generate_recommendations, combined_text)

                # ë‘ ìŠ¤íŠ¸ë¦¼ì—ì„œ ë„ì°©í•˜ëŠ” ëŒ€ë¡œ gRPCë¡œ ì „ë‹¬
                async def push_to_queue(tag, stream, out_queue):
                    async for chunk in stream:
                        await out_queue.put((tag, chunk))
                    await out_queue.put((tag, None))  # ì¢…ë£Œ ì‹ í˜¸

                output_queue = asyncio.Queue()

                # ë‘ LLM ìŠ¤íŠ¸ë¦¼ì„ ë³‘ë ¬ ì‹¤í–‰
                task2 = asyncio.create_task(push_to_queue("SONAR", sonar_stream, output_queue))
                task1 = asyncio.create_task(push_to_queue("FINAL_SUMMARY", watsonx_stream, output_queue))


                finished = set()
                while len(finished) < 2:
                    tag, chunk = await output_queue.get()
                    if chunk is None:
                        finished.add(tag)
                        continue
                    # ë©”ì‹œì§€ í¬ë§·ì— ë”°ë¼ êµ¬ë¶„ì ë¶™ì—¬ì„œ yield
                    if tag == "FINAL_SUMMARY":
                        yield docssummary_pb2.DocsSummaryResponse(line=f"FINAL_SUMMARY: {chunk}")
                    elif tag == "SONAR":
                        yield docssummary_pb2.DocsSummaryResponse(line=f"SONAR: {chunk}")

                await asyncio.gather(task1, task2)

        except Exception as e:
            print(f"Final summary ìƒì„± ì˜¤ë¥˜: {e}")
            yield docssummary_pb2.DocsSummaryResponse(
                line=f"FINAL_SUMMARY_ERROR: {str(e)}"
            )


async def serve():
    server = grpc.aio.server()
    docssummary_pb2_grpc.add_DocsSummaryServiceServicer_to_server(DocsSummaryService(), server)
    server.add_insecure_port('[::]:50053')
    await server.start()
    print("gRPC DocsSummaryService running on port 50053", flush=True)
    try:
        await server.wait_for_termination()
    except asyncio.CancelledError:
        print("gRPC server cancelled (shutting down cleanly)")


if __name__ == "__main__":
    try:
        asyncio.run(serve())
    except KeyboardInterrupt:
        print("Server stopped by user")