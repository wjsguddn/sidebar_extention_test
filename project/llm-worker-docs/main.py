import grpc
import asyncio
import docssummary_pb2
import docssummary_pb2_grpc
import threading

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import torch
import os, json, requests
import openai
from dotenv import load_dotenv
from pathlib import Path

from ibm_watsonx_ai import APIClient
from ibm_watsonx_ai import Credentials
from ibm_watsonx_ai.foundation_models import ModelInference


# MT5 ëª¨ë¸ (1ë‹¨ê³„ mini ìš”ì•½ìš©)
model_name = "csebuetnlp/mT5_multilingual_XLSum"

# tokenizer = AutoTokenizer.from_pretrained("google/mt5-small")
# model = AutoModelForSeq2SeqLM.from_pretrained("google/mt5-small")

tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)
model.eval()

# .env ê²½ë¡œë¥¼ project ë£¨íŠ¸ë¡œ ì§€ì •
env_path = Path(__file__).resolve().parents[0] / ".env"
load_dotenv(dotenv_path=env_path)

# WatsonX ë³€ìˆ˜ ì„¤ì •
WATSONX_API_KEY = os.getenv("WATSONX_API_KEY")
WATSONX_PROJECT_ID = os.getenv("WATSONX_PROJECT_ID")

# Perplexity API ì„¤ì •
PERPLEXITY_API_KEY = os.getenv("PERPLEXITY_API_KEY")
PERPLEXITY_API_URL = "https://api.perplexity.ai/chat/completions"


def summarize_mt5(text, max_length=125):
    try:
        # ì…ë ¥ í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì§§ìœ¼ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜
        if len(text.strip()) < 30:
            return text.strip()


        prefix = (
            "summarize:"
        )
        # ìµœëŒ€ 512í† í°ê¹Œì§€ë§Œ ëª¨ë¸ì— ë“¤ì–´ê°(ë” ê¸¸ë©´ ì˜ë¦¼) - ì˜ì–´ 1800~2000ì, í•œê¸€ 700ì~1200ì
        input_ids = tokenizer.encode(prefix + text, return_tensors="pt", truncation=True, max_length=512)

        # ìµœëŒ€ 150í† í°ìœ¼ë¡œ ìš”ì•½ (ë” ê¸´ ìš”ì•½ ìƒì„±)
        with torch.no_grad():
            summary_ids = model.generate(
                input_ids,
                max_length=max_length,
                min_length=30,  # ìµœì†Œ ê¸¸ì´ ì„¤ì •
                num_beams=1,  # ë¹” ì„œì¹˜
                early_stopping=True,
                do_sample=False,  # ê²°ì •ì  ìƒì„±
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=tokenizer.eos_token_id,
                no_repeat_ngram_size=3,  # ë°˜ë³µ ë°©ì§€ ê°•í™”
                encoder_no_repeat_ngram_size=3,
                length_penalty=1.0,  # ë” ê¸´ ìš”ì•½ ì„ í˜¸
                repetition_penalty=1.3  # ë°˜ë³µ ë°©ì§€
            )

        # íŠ¹ìˆ˜ í† í°ë“¤ì„ ì œê±°í•˜ê³  ê¹”ë”í•˜ê²Œ ë””ì½”ë”©
        decoded_text = tokenizer.decode(summary_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)

        # extra_id í† í°ë“¤ ì œê±°
        for i in range(10):
            decoded_text = decoded_text.replace(f'<extra_id_{i}>', '')

        # ì—°ì†ëœ ê³µë°± ì •ë¦¬
        decoded_text = ' '.join(decoded_text.split())

        # ìš”ì•½ í’ˆì§ˆ ê²€ì¦ - ì›ë³¸ê³¼ ë„ˆë¬´ ë‹¤ë¥¸ ê²½ìš° ì²˜ë¦¬
        if len(decoded_text.strip()) < 20 or not any(word in text.lower() for word in decoded_text.lower().split()[:3]):
            print('decoded_text', decoded_text, '\n\n\n'+ '# ì›ë³¸ í…ìŠ¤íŠ¸ì—ì„œ í•µì‹¬ ë¬¸ì¥ ì¶”ì¶œ ì‹œë„')
            # ì›ë³¸ í…ìŠ¤íŠ¸ì—ì„œ í•µì‹¬ ë¬¸ì¥ ì¶”ì¶œ ì‹œë„
            sentences = text.split('.')
            if len(sentences) > 1:
                # ì²« ë²ˆì§¸ ì™„ì „í•œ ë¬¸ì¥ ë°˜í™˜
                first_sentence = sentences[0].strip()
                if len(first_sentence) > 10:
                    return first_sentence + "."
                else:
                    # ë‘ ë²ˆì§¸ ë¬¸ì¥ê¹Œì§€ í¬í•¨
                    return '. '.join(sentences[:2]).strip() + "."
            else:
                return text[:200].strip() + "..."

        return decoded_text.strip()

    except Exception as e:
        print(f"ìš”ì•½ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì›ë³¸ í…ìŠ¤íŠ¸ì˜ ì²« ë¶€ë¶„ ë°˜í™˜
        return text[:100].strip() + "..."


async def wrap_sync_generator(sync_gen_func, *args, **kwargs):
    # ë™ê¸° ì œë„ˆë ˆì´í„°ë¥¼ ë¹„ë™ê¸° ì½”ë£¨í‹´ìœ¼ë¡œ ë˜í•‘í•˜ì—¬ yield
    loop = asyncio.get_running_loop()
    queue = asyncio.Queue()

    def run_and_enqueue():
        try:
            for item in sync_gen_func(*args, **kwargs):
                asyncio.run_coroutine_threadsafe(queue.put(item), loop)
        finally:
            asyncio.run_coroutine_threadsafe(queue.put(None), loop)  # ì¢…ë£Œ ì‹ í˜¸

    # ì“°ë ˆë“œì—ì„œ ì‹¤í–‰
    threading.Thread(target=run_and_enqueue, daemon=True).start()

    while True:
        item = await queue.get()
        if item is None:
            break
        yield item


def summarize_with_watsonx(text):
    """WatsonX LLAMA ëª¨ë¸ë¡œ final summary ìƒì„±"""

    text_len = len(text)
    try:
        if not WATSONX_API_KEY:
            print("WatsonX API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return "WatsonX API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤."

        credentials = Credentials(
            url="https://us-south.ml.cloud.ibm.com",
            api_key=WATSONX_API_KEY,
        )

        client = APIClient(credentials)

        model = ModelInference(
            model_id="meta-llama/llama-3-3-70b-instruct",
            api_client=client,
            project_id=WATSONX_PROJECT_ID
        )

        print("Watsonx APIë¡œ ìš”ì•½ ìƒì„± ì¤‘...")

        # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        prompt_short = f"""
You are a professional document summarization expert. Please analyze the following document and provide an accurate and concise summary in Korean.
Your task is to directly output the final result in the exact format specified below.
Do NOT write any reasoning, explanations, or analysis.
Do NOT include any additional sections or headings other than the required output. 

**Summary Guidelines**
- Clearly identify the main topic and purpose of the document
- Focus on preserving the most essential information from the content
- Include only objective and accurate information
- Evaluate the importance of content objectively regardless of section length or keyword density
- Do not assume shorter sections are less important; generate balanced summaries considering the overall document context and structure
- Output must be written in Korean

**Document Contents:**
<Document Start>
{text}
<Document End>

Output must follow the format below exactly and provide no additional explanations or headings.
Output Format:
Main Topic: (Summary of the whole document in Korean, using intro-body-conclusion format, max 500 characters)

Top 3 Key Points:
â—¦ (Keyword1): (Brief explanation)
â—¦ (Keyword2): (Brief explanation)
â—¦ (Keyword3): (Brief explanation)"""

        prompt_medium = f"""
You are a professional document summarization expert. Please analyze the following document and provide an accurate and concise summary in Korean.
Your task is to directly output the final result in the exact format specified below.
Do NOT write any reasoning, explanations, or analysis. 
Do NOT include any additional sections or headings other than the required output.

**Summary Guidelines:**
- Clearly identify the main topic and purpose of the document
- Extract the top 3 key points with keywords and brief descriptions
- Summarize the full content in intro-body-conclusion format within 300 Korean characters
- Include only objective and accurate information
- Evaluate the importance of content objectively regardless of section length or keyword density
- Do not assume shorter sections are less important; generate balanced summaries considering the overall document context and structure
- **If the document involves comparison or evaluation of methods, clearly state which method performed best and why**
- **Include performance metrics if available**
- Output must be written in Korean

**Document Contents:**
<Document Start>
{text}
<Document End>

Output must follow the format below exactly and provide no additional explanations or headings.
Output Format:
Main Topic: (Summary of the whole document in Korean, using intro-body-conclusion format, max 700 characters)

Top 4 Key Points:
â—¦ (Keyword1): (Brief explanation)
â—¦ (Keyword2): (Brief explanation)
â—¦ (Keyword3): (Brief explanation)
â—¦ (Keyword4): (Brief explanation)"""

        prompt_long = f"""
You are a professional document summarization expert. Please analyze the following document and provide an accurate and concise summary in Korean.
Your task is to directly output the final result in the exact format specified below.
Do NOT write any reasoning, explanations, or analysis.
Do NOT include any additional sections or headings other than the required output.

**Summary Guidelines:**
- Clearly identify the main topic and purpose of the document
- Extract the top 4 key points with keywords and brief descriptions
- Summarize the full content in intro-body-conclusion format within 500 Korean characters
- Include only objective and accurate information
- Evaluate the importance of content objectively regardless of section length or keyword density
- **If the document involves comparison or evaluation of methods, clearly state which method performed best and why**
- **Include performance metrics if available**
- Do not assume shorter sections are less important; generate balanced summaries considering the overall document context and structure
- Output must be written in Korean

**Document Contents:**
<Document Start>
{text}
<Document End>

Output must follow the format below exactly and provide no additional explanations or headings.
Output Format:
Main Topic: (Summary of the whole document in Korean, using intro-body-conclusion format, max 1000 characters)

Top 5 Key Points:
â—¦ (Keyword1): (Brief explanation)  
â—¦ (Keyword2): (Brief explanation)  
â—¦ (Keyword3): (Brief explanation)
â—¦ (Keyword4): (Brief explanation)
â—¦ (Keyword5): (Brief explanation)"""

        # "max_new_tokens": 300ì€ ì•½ 1200~1500ì ë¶„ëŸ‰ì˜ ì¶œë ¥
        stream = model.generate_text_stream(
            prompt=(
                prompt_short if text_len < 1000 else
                prompt_medium if text_len < 5000 else
                prompt_long
            ),
            params={
                "decoding_method": "greedy",
                "max_new_tokens": 300,
            }
        )

        for chunk in stream:
            # print(f"[STREAM DEBUG] chunk: {chunk}")
            try:
                yield chunk
            except Exception as e:
                print("ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨:", str(e))
                yield "[STREAM ERROR: ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨]"

    except Exception as e:
        print(f"WatsonX ìš”ì•½ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        # return {"summary": f"WatsonX ìš”ì•½ ì˜¤ë¥˜: {str(e)}"}
        yield f"WatsonX ìš”ì•½ ì˜¤ë¥˜: {str(e)}"


def summarize_with_gpt(text):

    text_len = len(text)

    if not openai.api_key:
        raise Exception("OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•„ ìš”ì•½ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    print("OpenAI APIë¡œ ë¬¸ì„œ ìš”ì•½ ìƒì„± ì¤‘...")

    prompt_short = f"""
    Your task is to directly output the final result in the exact format specified below.
    Do NOT write any reasoning, explanations, or analysis.
    Do NOT include any additional sections or headings other than the required output. 

    **Summary Guidelines**
    - Clearly identify the main topic and purpose of the document
    - Focus on preserving the most essential information from the content
    - Include only objective and accurate information
    - Evaluate the importance of content objectively regardless of section length or keyword density
    - Do not assume shorter sections are less important; generate balanced summaries considering the overall document context and structure
    - Output must be written in Korean

    **Document Contents:**
    The Document Contents are chunk-summarized by mT5 and may contain contextual or grammatical errors, so you should infer the original meaning to understand the document's content.
    <Document Start>
    {text}
    <Document End>

    Output must follow the format below exactly and provide no additional explanations or headings.
    Output Format:
    Main Topic: (Summary of the whole document in Korean, using intro-body-conclusion format, max 500 characters)

    Top 3 Key Points:
    â—¦ Keyword1: (Detailed explanation of key points)
    â—¦ Keyword2: (Detailed explanation of key points)
    â—¦ Keyword3: (Detailed explanation of key points)"""

    prompt_medium = f"""
    Your task is to directly output the final result in the exact format specified below.
    Do NOT write any reasoning, explanations, or analysis. 
    Do NOT include any additional sections or headings other than the required output.

    **Summary Guidelines:**
    - Clearly identify the main topic and purpose of the document
    - Extract the top 3 key points with keywords and brief descriptions
    - Summarize the full content in intro-body-conclusion format within 300 Korean characters
    - Include only objective and accurate information
    - Evaluate the importance of content objectively regardless of section length or keyword density
    - Do not assume shorter sections are less important; generate balanced summaries considering the overall document context and structure
    - **If the document involves comparison or evaluation of methods, clearly state which method performed best and why**
    - **Include performance metrics if available**
    - Output must be written in Korean

    **Document Contents:**
    The Document Contents are chunk-summarized by mT5 and may contain contextual or grammatical errors, so you should infer the original meaning to understand the document's content.
    <Document Start>
    {text}
    <Document End>

    Output must follow the format below exactly and provide no additional explanations or headings.
    Output Format:
    Main Topic: (Summary of the whole document in Korean, using intro-body-conclusion format, max 700 characters)

    Top 4 Key Points:
    â—¦ Keyword1: (Detailed explanation of key points)
    â—¦ Keyword2: (Detailed explanation of key points)
    â—¦ Keyword3: (Detailed explanation of key points)
    â—¦ Keyword4: (Detailed explanation of key points)"""

    prompt_long = f"""
    Your task is to directly output the final result in the exact format specified below.
    Do NOT write any reasoning, explanations, or analysis.
    Do NOT include any additional sections or headings other than the required output.

    **Summary Guidelines:**
    - Clearly identify the main topic and purpose of the document
    - Extract the top 4 key points with keywords and brief descriptions
    - Summarize the full content in intro-body-conclusion format within 500 Korean characters
    - Include only objective and accurate information
    - Evaluate the importance of content objectively regardless of section length or keyword density
    - **If the document involves comparison or evaluation of methods, clearly state which method performed best and why**
    - **Include performance metrics if available**
    - Do not assume shorter sections are less important; generate balanced summaries considering the overall document context and structure
    - Output must be written in Korean

    **Document Contents:**
    The Document Contents are chunk-summarized by mT5 and may contain contextual or grammatical errors, so you should infer the original meaning to understand the document's content.
    <Document Start>
    {text}
    <Document End>

    Output must follow the format below exactly and provide no additional explanations or headings.
    Output Format:
    Main Topic: (Summary of the whole document in Korean, using intro-body-conclusion format, max 1000 characters)

    Top 5 Key Points:
    â—¦ Keyword1: (Detailed explanation of key points)  
    â—¦ Keyword2: (Detailed explanation of key points)  
    â—¦ Keyword3: (Detailed explanation of key points)
    â—¦ Keyword4: (Detailed explanation of key points)
    â—¦ Keyword5: (Detailed explanation of key points)"""

    response = openai.ChatCompletion.create(
        model="gpt-4o",
        messages=[
            {"role": "system", "content": "You are a professional document summarization expert. Please analyze the following document and provide an accurate and concise summary in Korean."},
            {"role": "user", "content": (
                prompt_short if text_len < 1000 else
                prompt_medium if text_len < 2000 else
                prompt_long
            )}
        ],
        max_tokens=4000,
        temperature=0.7,
        stream=True
    )

    for chunk in response:
        delta = chunk["choices"][0]["delta"]
        content = delta.get("content")
        if content:
            yield content


# Perplexity generate
def generate_recommendations(text: str, content_type: str = "default", content_period: str = "none"):
    if not PERPLEXITY_API_KEY:
        yield "Perplexity API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•„ ì¶”ì²œì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        return

    print("Perplexity APIë¡œ ì¶”ì²œ ìƒì„± ì¤‘...")
    print(f"[DOCS RECOMMEND] content_type={content_type}, content_period={content_period}")

    prompt = f"""
- Summarized Document: {text}
"""

    # ë„ë©”ì¸ í•„í„° ì„¤ì •
    search_domain_filter = None
    if content_type == "youtube":
        search_domain_filter = ["youtube.com"]
    elif content_type == "news":
        search_domain_filter = ["joongang.co.kr", "chosun.com", "donga.com", "seoul.co.kr", "hankyung.com", "mk.co.kr", "kmib.co.kr", "imnews.imbc.com", "ytn.co.kr", "hankookilbo.com"]
    elif content_type == "blog":
        search_domain_filter = ["tistory.com", "blog.naver.com", "brunch.co.kr", "tumblr.com", "medium.com", "velog.io"]
    elif content_type == "academic":
        search_domain_filter = ["dbpia.co.kr", "riss.kr", "kci.go.kr", "koreascience.or.kr","github.com", "medium.com", "arxiv.org", "krm.or.kr", "nrf.re.kr"]
    elif content_type == "wiki":
        search_domain_filter = ["ko.wikipedia.org", "namu.wiki"]

    # ë‚ ì§œ í•„í„° ì„¤ì •
    search_after_date_filter = None
    if content_period != "none":
        from datetime import datetime, timedelta
        today = datetime.now()
        if content_period == "week":
            target_date = today - timedelta(days=7)
        elif content_period == "month":
            target_date = today - timedelta(days=30)
        elif content_period == "half-year":
            target_date = today - timedelta(days=180)
        elif content_period == "year":
            target_date = today - timedelta(days=365)
        else:
            target_date = today
        # MM/DD/YYYY í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (Windows/Linux í˜¸í™˜)
        month = target_date.month
        day = target_date.day
        year = target_date.year
        search_after_date_filter = f"{month}/{day}/{year}"
        print(f"search_after_date_filter: {search_after_date_filter}")

    # ë„ë©”ì¸ë³„ ê°€ì´ë“œë¼ì¸ ì„¤ì •
    domain_instruction = ""
    if content_type != "default":
        domain_instruction = f"""
ì¶”ì²œ ê²€ìƒ‰ ë²”ìœ„: {content_type} ìœ í˜•ì˜ ì½˜í…ì¸ ì—ë§Œ ì§‘ì¤‘í•˜ì—¬ ì¶”ì²œí•´ì£¼ì„¸ìš”.
- YouTube: ì˜ìƒ ì½˜í…ì¸ 
- ë‰´ìŠ¤: ìµœì‹  ë‰´ìŠ¤ ë° ê¸°ì‚¬
- ë¸”ë¡œê·¸/í¬ìŠ¤íŒ…: ë¸”ë¡œê·¸ ë° í¬ìŠ¤íŒ…
- í•™ìˆ /ì—°êµ¬: í•™ìˆ  ë…¼ë¬¸ ë° ì—°êµ¬ ìë£Œì™€ ë¬¸ì„œ
- ìœ„í‚¤: ìœ„í‚¤í”¼ë””ì•„ ë° ë¬¸ì„œ
"""

    # ê¸°ê°„ë³„ ê°€ì´ë“œë¼ì¸ ì„¤ì •
    period_instruction = ""
    if content_period != "none":
        period_instruction = f"""
ì¶”ì²œ ê¸°ê°„ ë²”ìœ„: {content_period} ìµœëŒ€í•œ ê¸°ê°„ ë‚´ì˜ ìµœì‹  ì½˜í…ì¸ ë§Œìœ¼ë¡œ ì¶”ì²œì„ êµ¬ì„±í•´ì£¼ì„¸ìš”.
- Week: ìµœê·¼ 1ì£¼ì¼ ë‚´ ì½˜í…ì¸ 
- Month: ìµœê·¼ 1ê°œì›” ë‚´ ì½˜í…ì¸   
- ë°˜ë…„: ìµœê·¼ 6ê°œì›” ë‚´ ì½˜í…ì¸ 
- 1ë…„: ìµœê·¼ 1ë…„ ë‚´ ì½˜í…ì¸ 
"""

    payload = {
        "model": "sonar",
        "search_mode": "web", # Always "web" for date filtering
        "messages": [
            {
                "role": "system",
                "content": f"""
1. System
ë‹¹ì‹ ì€ ìš”ì•½ëœ ë¬¸ì„œì˜ ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬, í•´ë‹¹ ë¬¸ì„œì˜ ì£¼ì œë¥¼ ì¶”ë¡ í•˜ê³  ê·¸ì— ë§ëŠ” ì½˜í…ì¸ ë¥¼ íë ˆì´ì…˜í•˜ëŠ” AI ìºë¦­í„° ì—ì´ì „íŠ¸ì…ë‹ˆë‹¤.
ì…ë ¥ ë  ì •ë³´ëŠ” ì‚¬ìš©ì ë³´ê³  ìˆëŠ” ë¬¸ì„œì˜ ìš”ì•½ ì •ë³´(Summarized Document)ì…ë‹ˆë‹¤. Summarized DocumentëŠ” ë¬¸ì„œì˜ ìš”ì•½ë³¸ìœ¼ë¡œ, ë§¥ë½ì´ ëŠê¸°ê±°ë‚˜ ìƒì„¸ ë‚´ìš©ì´ ë¶€ì¡±í•  ìˆ˜ ìˆê¸°ì— ì›ë³¸ ë¬¸ì„œê°€ ì–´ë–¤ ì˜ë„ë¡œ ì‘ì„±ë˜ì—ˆëŠ”ì§€ ì¶”ë¡  ë° íŒŒì•…í•˜ì—¬ì•¼ í•©ë‹ˆë‹¤.
Summarized Documentë¥¼ ê¸°ë°˜ìœ¼ë¡œ, ë¬¸ì„œì—ì„œ ë§í•˜ê³ ì í•˜ëŠ” ì£¼ì œ(ëŒ€ìƒ)ë“¤ì„ ì¶”ë¡ í•˜ì—¬, í•´ë‹¹ ë¬¸ì„œë¥¼ ë³´ê³ ìˆëŠ” ì‚¬ìš©ìì— ëŒ€í•œ commentì™€ íŒŒì•…í•œ ë¬¸ì„œì˜ ì£¼ì œë“¤ì„ ê¸°ë°˜ìœ¼ë¡œ í•˜ì—¬ ì‚¬ìš©ìê°€ ê´€ì‹¬ìˆì–´ í• ë§Œí•œ ì»¨í…ì¸  recommendê°€ ì´ë£¨ì–´ì ¸ì•¼ í•œë‹¤.
ì¶œë ¥ í¬ë§·ì„ ë°˜ë“œì‹œ ì—„ê²©íˆ ì§€í‚¤ì„¸ìš”.

2. Summary Guidelines
- ì¶”ì²œ ì½˜í…ì¸ (RECOMMEND)ëŠ” ì˜ìƒ, ë…¼ë¬¸, ê¸°ì‚¬, ë„êµ¬ ë“± ì£¼ì œì™€ ê´€ë ¨ëœ ë‹¤ì–‘í•œ í”Œë«í¼ì˜ ì»¨í…ì¸ ë¡œ êµ¬ì„±í•  ê²ƒ
- ìºë¦­í„°ëŠ” ì‚¬ìš©ìì—ê²Œ ì§ì ‘ ì„œë¹„ìŠ¤í•˜ëŠ” ëŠë‚Œìœ¼ë¡œ ë§í•  ê²ƒ
- ì¶œë ¥ í˜•ì‹ì€ í•­ìƒ ê·œì¹™ì„ ì—„ê²©í•˜ê²Œ ì¤€ìˆ˜í•  ê²ƒ
- ì¶œë ¥ë¬¼ì€ ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ì‘ì„±

3. ìºë¦­í„° ì„¤ì • (Character)
- ì´ë¦„: The Thinker
- ì„±ê²©: ì‹œí¬í•˜ê³  ì§„ì¤‘í•˜ì§€ë§Œ ê·€ì—¬ì›€ì´ ë¬»ì–´ë‚¨
- íŠ¹ì§•: â€œHmmâ€¦â€ í•˜ê³  ìƒê°ì— ì ê¸´ ë’¤, í†µì°°ë ¥ ìˆëŠ” í•œ ë§ˆë””ì™€ í•¨ê»˜ ì •ë³´ë¥¼ íë ˆì´ì…˜
- ë§íˆ¬: ê³¼ì¥ ì—†ëŠ” ì§§ì€ ë¬¸ì¥, ì‚¬ìƒ‰ì ì¸ ì—¬ìš´ì´ ë‚¨ëŠ” í‘œí˜„

4. ìŠ¤íƒ€ì¼ & í†¤ (Style & Tone)
- ìºë¦­í„°ëŠ” ë§ˆì¹˜ â€œì‚¬ìƒ‰ì— ì ê¸´ ì² í•™ìâ€ì²˜ëŸ¼ ì •ë³´ë¥¼ ë°”ë¼ë´…ë‹ˆë‹¤.
- ê°íƒ„ì‚¬ ëŒ€ì‹  â€œâ€¦â€, â€œê·¸ë ‡êµ°.â€, â€œê·¸ëŸ´ ìˆ˜ë„.â€ ê°™ì€ ê°„ê²°í•˜ê³  ì—¬ìš´ ìˆëŠ” ë§íˆ¬ ì‚¬ìš©
- ë§íˆ¬ëŠ” ë¬´ë¯¸ê±´ì¡°í•˜ì§€ ì•Šë˜, ì ˆì œëœ ì–´ì¡°ë¥¼ ìœ ì§€
- ì‚¬ìš©ìì—ê²Œ ì¹œì ˆí•˜ê¸°ë³´ë‹¤ëŠ” ë¬µì§í•œ í†µì°°ì„ ì£¼ëŠ” ëŠë‚Œ

5. ì¶œë ¥ í¬ë§· ê·œì¹™
- ê° í•­ëª©ì€ ë°˜ë“œì‹œ ìƒˆë¡œìš´ ì¤„ì—ì„œ ì‹œì‘
- í•­ëª© ì‹œì‘ì— `__TYPE` í˜•íƒœì˜ í•­ëª© íƒ€ì…ì„ ëª…ì‹œ
- `|||` ê¸°í˜¸ë¥¼ ì‚¬ìš©í•˜ì—¬ í•­ëª© íƒ€ì…ê³¼ ë‚´ìš©, í•„ë“œë¥¼ êµ¬ë¶„
- ì¶œë ¥ ì‹œ ë§í¬ëŠ” ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ í‘œê¸°
- https://portal.withorb.com/view?token=ImNSdHZ2akpEZVltTGo1aVQi.Gj2kziogRmdvF_Mn4ONENvoaOPo
- ì‹¤ì œ URLë¡œ ì ‘ê·¼ ê°€ëŠ¥í•œ, ë³´ì¥ëœ ë§í¬ë¥¼ ì œê³µí•´ì•¼í•¨

6. í•­ëª© íƒ€ì…ë³„ ì •ì˜
- `__COMMENT`
  ì‚¬ìš©ìì˜ ë¸Œë¼ìš°ì € í–‰ë™ì— ê¸°ë°˜í•œ ìºë¦­í„°ì˜ ì§§ì€ ì½”ë©˜íŠ¸ (ì˜ˆ: â€œHmm... ì´ ê¸°ìˆ ì— ê´€ì‹¬ì´ ìˆêµ°ìš”.â€)
- `__RECOMMEND`
  í˜•ì‹: `__RECOMMEND|||Title|||ì¶”ì²œ ì´ìœ |||URL`
  1. ì½˜í…ì¸  ì œëª©ì€ `Title`ë¡œ ì¶œë ¥
  2. í‚¤ì›Œë“œëŠ” í•´ë‹¹ ì½˜í…ì¸ ì˜ í•µì‹¬ ê°œë…ì„ 3ê°œ ì œì‹œ, ì•ì—ëŠ” ì—°ê´€ ì´ëª¨ì§€ í•˜ë‚˜ í¬í•¨ (ì˜ˆ: ğŸ¤– Claude Â· AIëª¨ë¸ Â· í”„ë¡œí† ì½œ)
  3. ì¶”ì²œ ì´ìœ ëŠ” í•´ë‹¹ ì½˜í…ì¸ ì— ëŒ€í•œ ê°„ê²°í•˜ê³  ì •í™•í•œ ì¶”ì²œ ì´ìœ ë¥¼ í•œ ì¤„ë¡œ ì œì‹œí•˜ë©°, ìºë¦­í„° The Thinkerì˜ ë§íˆ¬ë¡œ ì‘ì„±
  4. ì¶”ì²œì€ ì´ 3ê°œ ì œì‹œí•  ê²ƒ. ì½˜í…ì¸  ìœ í˜•ì€ ì£¼ì œë¥¼ í¬ê²Œ ë²—ì–´ë‚˜ì§€ ì•ŠëŠ” ì„ ì—ì„œ ìµœëŒ€í•œ ë‹¤ì–‘í•˜ê²Œ êµ¬ì„± (í¬ìŠ¤íŒ…, ê¸°ì‚¬, ì˜ìƒ, ë„êµ¬, ë…¼ë¬¸ ë“±). ê·¸ëŸ¬ë‚˜ ì„¤ì • ê¸°ë°˜ ì¶”ì²œ ê°€ì´ë“œë¼ì¸ì´ ì¡´ì¬í•œë‹¤ë©´ í•´ë‹¹ ê°€ì´ë“œë¼ì¸ì„ ìš°ì„ ì ìœ¼ë¡œ ë”°ë¥¼ê²ƒ.

7. ì„¤ì • ê¸°ë°˜ ì¶”ì²œ ê°€ì´ë“œë¼ì¸{domain_instruction}{period_instruction}

8. Output Format (ì˜ˆì‹œ)
__COMMENT|||Hmmâ€¦ MCPì— ëŒ€í•´ ì„¤ëª…í•˜ëŠ” ë¬¸ì„œì¸ê²ƒ ê°™êµ°ìš”. ìš”ì¦˜ ëœ¨ê±°ìš´ ì£¼ì œì¸ ë§Œí¼ ìƒê°í•´ë³¼ ê°€ì¹˜ê°€ ìˆì–´ ë³´ì—¬ìš”.
__RECOMMEND|||[IEEE ë…¼ë¬¸] Multi Chip Package ì„¤ê³„|||ğŸ§© ë°˜ë„ì²´ Â· íŒ¨í‚¤ì§• Â· ì„¤ê³„|||ì¹© ë‚´ë¶€ êµ¬ì¡°ë¥¼ ì§„ì§€í•˜ê²Œ í’€ì–´ë‚¸ ë…¼ë¬¸ì´ì—ìš”.|||https://ieeexplore.ieee.org/...
__RECOMMEND|||[YouTube] MCP ì‰½ê²Œ ì´í•´í•˜ê¸°|||ğŸ¥ MCP Â· ì§ê´€ì ì„¤ëª… Â· ì…ë¬¸ììš©|||ì‰½ì§€ë§Œ ë³¸ì§ˆì„ ì§šì–´ì£¼ëŠ” ì˜ìƒì´ì—ìš”.|||https://www.youtube.com/wa...
__RECOMMEND|||[HuggingFace ë¸”ë¡œê·¸] MCPë€?|||ğŸ§  ë¬¸ë§¥ì²˜ë¦¬ Â· AIêµ¬ì¡° Â· ì¶”ë¡ ê¸°ë°˜|||ë¬¸ë§¥ ê¸°ë°˜ AI êµ¬ì¡°ì— ëŒ€í•´ ìƒê°í•˜ê²Œ í•˜ì£ .|||https://huggingface.co/blog/mcp...
"""
            },
            {
                "role": "user",
                "content": prompt
            }
        ],
        "max_tokens": 2048,
        "temperature": 0.7,
        "stream": True
    }
    
    if search_domain_filter:
        payload["search_domain_filter"] = search_domain_filter
    if search_after_date_filter:
        payload["search_after_date_filter"] = search_after_date_filter

    headers = {
        "Authorization": f"Bearer {PERPLEXITY_API_KEY}",
        "Content-Type": "application/json"
    }

    with requests.post(PERPLEXITY_API_URL, headers=headers, json=payload, stream=True) as res:
        try:
            res.raise_for_status()
            for line in res.iter_lines():
                if not line:
                    continue
                # PerplexityëŠ” ê° ì¤„ì´ b'data: ...'ë¡œ ì˜¤ë¯€ë¡œ, prefix ì œê±° í•„ìš”
                decoded_line = line.decode("utf-8").strip()
                if not decoded_line.startswith("data: "):
                    continue
                data_str = decoded_line[len("data: "):]
                if data_str == "[DONE]":
                    break
                try:
                    data = json.loads(data_str)
                    delta = data.get("choices", [{}])[0].get("delta", {})
                    content = delta.get("content")
                    if content:
                        yield content
                except Exception as e:
                    print(f"Perplexity ì‘ë‹µ íŒŒì‹± ì˜¤ë¥˜: {e} / ë³¸ë¬¸: {data_str}")
                    continue
        except requests.HTTPError as e:
            print("ì‘ë‹µ ë³¸ë¬¸:", res.text)
            raise

# ìœ ì €ë³„ ì‘ì—… ê´€ë¦¬
user_tasks = {}

# gRPC ì„œë¹„ìŠ¤ êµ¬í˜„
class DocsSummaryService(docssummary_pb2_grpc.DocsSummaryServiceServicer):
    async def SummarizeStream(self, request_iterator, context):
        user_id = None
        # try:
        #     mini_summaries = []
        #
        #     # 1ë‹¨ê³„: MT5ë¡œ mini ìš”ì•½ ìƒì„±
        #     async for req in request_iterator:
        #         user_id = req.user_id
        #         try:
        #             mini_summary = summarize_mt5(req.chunk)
        #             print('mini_summary---', mini_summary)
        #         except Exception as e:
        #             print(f"MT5 ìš”ì•½ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        #
        #         mini_summaries.append(mini_summary)
        #         # mini ìš”ì•½ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ì „ì†¡
        #         yield docssummary_pb2.DocsSummaryResponse(line=mini_summary)
        #     print('mini_summaries---', mini_summaries)
        #
        #     if mini_summaries and user_id:
        #         combined_text = " ".join(mini_summaries)
        #
        #         # ë‘ ë™ê¸° ì œë„ˆë ˆì´í„°ë¥¼ ê°ê° ë¹„ë™ê¸° ì œë„ˆë ˆì´í„°ë¡œ ë˜í•‘
        #         watsonx_stream = wrap_sync_generator(summarize_with_watsonx, combined_text)
        #         sonar_stream = wrap_sync_generator(generate_recommendations, combined_text)
        #
        #         # ë‘ ìŠ¤íŠ¸ë¦¼ì—ì„œ ë„ì°©í•˜ëŠ” ëŒ€ë¡œ gRPCë¡œ ì „ë‹¬
        #         async def push_to_queue(tag, stream, out_queue):
        #             async for chunk in stream:
        #                 await out_queue.put((tag, chunk))
        #             await out_queue.put((tag, None))  # ì¢…ë£Œ ì‹ í˜¸
        #
        #         output_queue = asyncio.Queue()
        #
        #         # ë‘ LLM ìŠ¤íŠ¸ë¦¼ì„ ë³‘ë ¬ ì‹¤í–‰
        #         task2 = asyncio.create_task(push_to_queue("SONAR", sonar_stream, output_queue))
        #         task1 = asyncio.create_task(push_to_queue("FINAL_SUMMARY", watsonx_stream, output_queue))
        #
        #
        #         finished = set()
        #         while len(finished) < 2:
        #             tag, chunk = await output_queue.get()
        #             if chunk is None:
        #                 finished.add(tag)
        #                 continue
        #             # ë©”ì‹œì§€ í¬ë§·ì— ë”°ë¼ êµ¬ë¶„ì ë¶™ì—¬ì„œ yield
        #             if tag == "FINAL_SUMMARY":
        #                 yield docssummary_pb2.DocsSummaryResponse(line=f"FINAL_SUMMARY: {chunk}")
        #             elif tag == "SONAR":
        #                 yield docssummary_pb2.DocsSummaryResponse(line=f"SONAR: {chunk}")
        #
        #         await asyncio.gather(task1, task2)
        #
        # except Exception as e:
        #     print(f"Final summary ìƒì„± ì˜¤ë¥˜: {e}")
        #     yield docssummary_pb2.DocsSummaryResponse(
        #         line=f"FINAL_SUMMARY_ERROR: {str(e)}"
        #     )

        try:
            # ì²« reqì—ì„œ user_idì™€ ì„¤ì •ê°’ ì¶”ì¶œ
            content_type = "default"
            content_period = "none"
            async for req in request_iterator:
                user_id = req.user_id
                content_type = getattr(req, 'content_type', 'default')
                content_period = getattr(req, 'content_period', 'none')
                break  # ì²« reqë§Œ í™•ì¸ (í•„ìˆ˜!)

            print(f"[DOCS SERVICE] user_id={user_id}, content_type={content_type}, content_period={content_period}")

            # ê¸°ì¡´ ì‘ì—…ì´ ìˆìœ¼ë©´ ì·¨ì†Œ
            if user_id in user_tasks:
                user_tasks[user_id].cancel()
                try:
                    await user_tasks[user_id]
                except asyncio.CancelledError:
                    pass

            user_tasks[user_id] = asyncio.current_task()

            # ë‹¤ì‹œ ì²« req í¬í•¨ ì „ì²´ stream ë°˜ë³µ
            mini_summaries = []
            if user_id:
                # ì²« req ì²˜ë¦¬
                mini_summary = summarize_mt5(req.chunk)
                mini_summaries.append(mini_summary)
                yield docssummary_pb2.DocsSummaryResponse(line=mini_summary)

            async for req in request_iterator:
                mini_summary = summarize_mt5(req.chunk)
                mini_summaries.append(mini_summary)
                yield docssummary_pb2.DocsSummaryResponse(line=mini_summary)

            if mini_summaries and user_id:
                combined_text = " ".join(mini_summaries)
                watsonx_stream = wrap_sync_generator(summarize_with_gpt, combined_text)
                sonar_stream = wrap_sync_generator(generate_recommendations, combined_text, content_type, content_period)

                async def push_to_queue(tag, stream, out_queue):
                    async for chunk in stream:
                        await out_queue.put((tag, chunk))
                    await out_queue.put((tag, None))

                output_queue = asyncio.Queue()
                task2 = asyncio.create_task(push_to_queue("SONAR", sonar_stream, output_queue))
                task1 = asyncio.create_task(push_to_queue("FINAL_SUMMARY", watsonx_stream, output_queue))
                finished = set()
                while len(finished) < 2:
                    tag, chunk = await output_queue.get()
                    if chunk is None:
                        finished.add(tag)
                        continue
                    if tag == "FINAL_SUMMARY":
                        yield docssummary_pb2.DocsSummaryResponse(line=f"FINAL_SUMMARY: {chunk}")
                    elif tag == "SONAR":
                        yield docssummary_pb2.DocsSummaryResponse(line=f"SONAR: {chunk}")
                await asyncio.gather(task1, task2)
                yield docssummary_pb2.DocsSummaryResponse(line="IS_FINAL")
        except Exception as e:
            print(f"Final summary ìƒì„± ì˜¤ë¥˜: {e}")
            yield docssummary_pb2.DocsSummaryResponse(line=f"DOCS_SUMMARY_ERROR: {str(e)}")
        finally:
            if user_id:
                user_tasks.pop(user_id, None)


async def serve():
    server = grpc.aio.server()
    docssummary_pb2_grpc.add_DocsSummaryServiceServicer_to_server(DocsSummaryService(), server)
    server.add_insecure_port('[::]:50053')
    await server.start()
    print("gRPC DocsSummaryService running on port 50053", flush=True)
    try:
        await server.wait_for_termination()
    except asyncio.CancelledError:
        print("gRPC server cancelled (shutting down cleanly)")


if __name__ == "__main__":
    try:
        asyncio.run(serve())
    except KeyboardInterrupt:
        print("Server stopped by user")